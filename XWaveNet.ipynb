{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Bidirectional, Dense\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import joblib\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b05efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file_path):\n",
    "  \n",
    "    #function that preprocesses the data from the dataset#\n",
    "    df = pd.read_csv(file_path)\n",
    "  \n",
    "    # Remove all NaN-containing entries\n",
    "    df = df.dropna()\n",
    "  \n",
    "    # Interpolate the numerical columns\n",
    "    numCols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    \n",
    "    df[numCols] = df[numCols].interpolate(method='linear', limit_direction ='forward')\n",
    "    \n",
    "    df[numCols] = df[numCols].interpolate(method='linear', limit_direction ='backward') \n",
    "    \n",
    "    \n",
    "    # Reorder columns\n",
    "    df = df.reindex(columns=cols)\n",
    "\n",
    "    # Set index and resample to a 2hour window\n",
    "    df = df.set_index('Time').resample('1H').median()\n",
    "    \n",
    "    # Remove all NaN-containing entries after resampling\n",
    "    df = df.dropna()\n",
    "       \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0744924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(train, test):\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    train_scaled = scaler.fit_transform(train.iloc[:, :11].values)\n",
    "    val_scaled = scaler.transform(test.iloc[:, :11].values)\n",
    "    \n",
    "    return train_scaled, val_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f5ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b25822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_history(history, title):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(loss))\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e86fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_steps(length):\n",
    "    return list(range(-length, 0))\n",
    "\n",
    "def multi_step_plot(history, true_future, prediction):\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    num_in = create_time_steps(len(history))\n",
    "    num_out = len(true_future)\n",
    "\n",
    "    plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
    "    plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
    "           label='True Future')\n",
    "    if prediction.any():\n",
    "        plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',\n",
    "                 label='Predicted Future')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61079b3",
   "metadata": {},
   "outputs": [],
   "source": [
    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5100a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictWaves(train_data_multi, val_data_multi, x_train_multi, future_steps, EPOCHS, EVALUATION_INTERVAL, PATIENCE):\n",
    "    multi_step_model = tf.keras.models.Sequential()\n",
    "    multi_step_model.add(tf.keras.layers.LSTM(64,\n",
    "                                          return_sequences = True,\n",
    "                                          input_shape = x_train_multi.shape[-2:]))\n",
    "    multi_step_model.add(tf.keras.layers.LeakyReLU(alpha=0.5)) \n",
    "    multi_step_model.add(tf.keras.layers.LSTM(32, return_sequences=True))\n",
    "    multi_step_model.add(tf.keras.layers.LeakyReLU(alpha=0.5)) \n",
    "    multi_step_model.add(tf.keras.layers.Dropout(0.2)) \n",
    "    multi_step_model.add(tf.keras.layers.LSTM(16, return_sequences=False))\n",
    "    multi_step_model.add(tf.keras.layers.Dropout(0.2))\n",
    "    multi_step_model.add(tf.keras.layers.Dense(future_steps))\n",
    "\n",
    "    print(multi_step_model.summary())\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  patience = PATIENCE,\n",
    "                                                  mode='min')\n",
    "    optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "    multi_step_model.compile(loss = tf.losses.MeanSquaredError(),\n",
    "                         optimizer = optimizer,\n",
    "                         metrics = [tf.metrics.MeanAbsoluteError()])\n",
    "    \n",
    "    multi_step_history = multi_step_model.fit(train_data_multi,\n",
    "                                          epochs=EPOCHS,\n",
    "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                          validation_data=val_data_multi,\n",
    "                                          validation_steps=EVALUATION_INTERVAL,\n",
    "                                          callbacks=[early_stopping])\n",
    "    \n",
    "    # plot_train_history(multi_step_history, 'Multi-Step Training and validation loss')\n",
    "    \n",
    "    histDF = pd.DataFrame(multi_step_history.history)\n",
    "    \n",
    "    return multi_step_model, histDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b30567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredvsAct(test_scaled_01):\n",
    "\n",
    "    x_test_multi_01, y_test_multi_01 = multivariate_data(test_scaled_01, test_scaled_01[:, 0], \n",
    "                                                         0, None, past_lags, future_steps, STEP)\n",
    "\n",
    "    y_tr = [y[0] for y in y_test_multi_01]\n",
    "    y_tr_hat = [x[0] for x in waveModel.predict(x_test_multi_01)]\n",
    "\n",
    "    df_tr_act = pd.concat([pd.DataFrame(y_tr), pd.DataFrame(x_test_multi_01[:,0,1:])],axis=1)\n",
    "    df_tr_act_inv = scaler.inverse_transform(df_tr_act)\n",
    "    df_tr_act_invDF = pd.DataFrame(df_tr_act_inv, columns = feature_list[:11])\n",
    "\n",
    "    df_tr_pred = pd.concat([pd.DataFrame(y_tr_hat), pd.DataFrame(x_test_multi_01[:,0,1:])],axis=1)\n",
    "    df_tr_pred_inv = scaler.inverse_transform(df_tr_pred)\n",
    "    df_tr_pred_invDF = pd.DataFrame(df_tr_pred_inv, columns = feature_list[:11])\n",
    "\n",
    "    times = test_01.reset_index()['Time'].values[-len(y_tr):]\n",
    "    \n",
    "    df_tr_pred_act = pd.concat([pd.DataFrame(times), pd.DataFrame(df_tr_pred_invDF['WVHT']), \n",
    "                               pd.DataFrame(df_tr_act_invDF['WVHT'])], axis=1)\n",
    "\n",
    "    df_tr_pred_act.columns = ['Time', 'WVHT_pred', 'WVHT_act']\n",
    "    \n",
    "    return df_tr_pred_act, times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515e7d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results_dataset(predictions, ci):\n",
    "    df = pd.DataFrame()\n",
    "    df['prediction'] = predictions\n",
    "    if ci >= 0:\n",
    "        df['upper'] = predictions + ci\n",
    "        df['lower'] = predictions - ci\n",
    "    else:\n",
    "        df['upper'] = predictions - ci\n",
    "        df['lower'] = predictions + ci\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead15a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predIntervals(df_tr_pred_act, alpha, times):    \n",
    "    \n",
    "    residuals = df_tr_pred_act['WVHT_act'] - df_tr_pred_act['WVHT_pred']\n",
    "\n",
    "    ci = np.quantile(residuals, 1 - alpha)\n",
    "\n",
    "    df_conf_01 = generate_results_dataset(pd.DataFrame(df_tr_pred_act['WVHT_pred']), ci)\n",
    "\n",
    "    df_conf_01 = pd.DataFrame(df_conf_01)\n",
    "\n",
    "    df_pred_act_01 = pd.concat([pd.DataFrame(times), pd.DataFrame(df_tr_pred_act['WVHT_pred']), \n",
    "                               \n",
    "                                pd.DataFrame(df_tr_pred_act['WVHT_act']), df_conf_01[['upper', 'lower']]], axis=1)\n",
    "\n",
    "    df_pred_act_01.columns = ['Time', 'WVHT_pred', 'WVHT_act', 'upper', 'lower']\n",
    "\n",
    "    df_pred_act_index_01 = df_pred_act_01.set_index('Time')\n",
    "    \n",
    "    return df_pred_act_index_01, residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82daab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot(df_pred_act_index_01, residuals):\n",
    "    \n",
    "    # calculate prediction metrics\n",
    "    d = df_pred_act_index_01['WVHT_act'].values - df_pred_act_index_01['WVHT_pred'].values\n",
    "    mse_f = np.mean(d**2)\n",
    "    mae_f = np.mean(abs(d))\n",
    "    rmse_f = np.sqrt(mse_f)\n",
    "    \n",
    "    forecastMetrics = {\"MAE\": mae_f, \"MSE\": mse_f, \"RMSE\": rmse_f}\n",
    "\n",
    "    # Calculate prediction intervals to plot  \n",
    "    RMSFE = np.sqrt(sum([x**2 for x in residuals]) / len(residuals))\n",
    "\n",
    "    band_size = 1.96*RMSFE\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "    ax.plot(df_pred_act_index_01.index, df_pred_act_index_01['WVHT_act'], c='black')\n",
    "\n",
    "    ax.plot(df_pred_act_index_01.index, df_pred_act_index_01['WVHT_pred'], c='red')\n",
    "\n",
    "    ax.fill_between(df_pred_act_index_01.index, (df_pred_act_index_01['WVHT_act'] - band_size), \n",
    "                    ( df_pred_act_index_01['WVHT_act']+band_size), color='g', alpha=.15)\n",
    "\n",
    "    ax.set_title(str(future_steps) + \"h ahead predictions w/ 95% Confidence\")\n",
    "\n",
    "    ax.set_xlabel('')\n",
    "\n",
    "    ax.set_ylabel('WVHT (m)')\n",
    "\n",
    "    ax.legend(('act','pred'), fontsize=\"15\")\n",
    "    \n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(15)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return forecastMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictExt(df, exthresh):\n",
    "    cv_data = df.copy()\n",
    "\n",
    "    # Add extreme-value column, with 1 or 0 for > or < exthresh\n",
    "    cv_data['Extreme_True'] = np.where(cv_data['WVHT'] > exthresh, 1, 0)\n",
    "\n",
    "    cv_data['Extreme_True'] = cv_data['Extreme_True'].astype('category')\n",
    "\n",
    "    label = cv_data[\"Extreme_True\"]\n",
    "\n",
    "    class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(label), y = label)\n",
    "\n",
    "    numCols = cv_data.select_dtypes(include = np.number).columns.tolist()\n",
    "\n",
    "    for col in numCols: \n",
    "        scaler = MinMaxScaler()\n",
    "        cv_data[col] = scaler.fit_transform(cv_data[col].values.reshape(-1,1))\n",
    "\n",
    "    X_train, y_train = cv_data.iloc[:, 0 : cv_data.shape[1]-1], label\n",
    "\n",
    "    ## Define the RF Classifier Model ##\n",
    "    clf = RandomForestClassifier(n_estimators=1000,   \n",
    "                                 max_depth=500,\n",
    "                                 max_features='auto', \n",
    "                                 bootstrap=True,\n",
    "                                 oob_score=True,\n",
    "                                 n_jobs=-1,\n",
    "                                 random_state=0,\n",
    "                                 verbose=0,\n",
    "                                 warm_start=False,\n",
    "                                 class_weight='balanced') # class_weights = array([ 0.51540202, 16.73163418])\n",
    "\n",
    "    ## Fit the model ##\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d76af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotExtCM(cfm_unseen, future_steps, labels, op_path):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = plt.axes()\n",
    "    sns.set(font_scale = 2)\n",
    "    sns.heatmap(cfm_unseen, annot=labels, fmt='', cmap='Blues', ax=ax)\n",
    "    ax.set_title(f\"Confusion matrix for {str(future_steps)+'hr ahead'}\")\n",
    "    ax.set_xlabel('Predicted Class',fontweight ='bold', fontsize=20, labelpad=10)\n",
    "    ax.set_ylabel('True Class',fontweight ='bold', fontsize=20, labelpad=10)\n",
    "    plt.savefig(op_path + str(future_steps)+'h_RF_CFM.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e67a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE PARAMETERS ###\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 13\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Data Loader Parameters\n",
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 10000\n",
    "TRAIN_SPLIT = train.shape[0]\n",
    "\n",
    "# LSTM and PI Parameters\n",
    "EVALUATION_INTERVAL = 500\n",
    "EPOCHS = 100\n",
    "PATIENCE = 5\n",
    "STEP = 1\n",
    "past_lags = 120\n",
    "alpha = 0.05\n",
    "\n",
    "# user-defined\n",
    "future_steps = 1\n",
    "exthresh = 2.0\n",
    "ip_path = './XWaveNet/Data/'\n",
    "op_path = './XWaveNet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f368e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL TRAINING ###\n",
    "\n",
    "## Define feature list ##\n",
    "feature_list = ['WVHT', 'WSPD', 'WDIR', 'ATMP', 'RH', 'PRES','APD', 'day_cos', 'day_sin', 'month_cos', 'month_sin']\n",
    "\n",
    "## Read and split data ##\n",
    "df = preprocess_data(ip_path + 'train_val.csv')\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.1, shuffle=False)\n",
    "\n",
    "\n",
    "## Transform data ##\n",
    "train_scaled, val_scaled = transform_data(train, test)\n",
    "\n",
    "x_train_multi, train_data_multi, val_data_multi = modelInput(train_scaled, val_scaled, past_lags, future_steps, STEP)\n",
    "\n",
    "## Forecast wave heights using the LSTM model ##\n",
    "waveModel, trainHistory = predictWaves(train_data_multi, val_data_multi, x_train_multi, \n",
    "                               future_steps, EPOCHS, EVALUATION_INTERVAL, PATIENCE)\n",
    "\n",
    "## save wave height forecast model ##\n",
    "waveModel.save(ip_path + str(future_steps) + \"h_wavModel.h5\")\n",
    "print('Model Saved!')\n",
    " \n",
    "retrain = 0\n",
    "if retrain == 0:\n",
    "    waveModel = load_model(ip_path + str(future_steps) + \"h_wavModel.h5\")\n",
    "else:\n",
    "    waveModel = waveModel        \n",
    "waveModel.summary()\n",
    "\n",
    "## Predict Extreme Events ##\n",
    "extClassifier = predictExt(df, exthresh)\n",
    "\n",
    "## save extreme event prediction model ##\n",
    "joblib.dump(extClassifier, op_path + '_extClassifier.joblib')\n",
    "\n",
    "retrainExt = 0\n",
    "if retrainExt == 0:\n",
    "    _extClassifier = joblib.load(op_path + '_extClassifier.joblib') # No need to initialize the model\n",
    "else:\n",
    "    _extClassifier = extClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b2573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREDICTION ON TEST DATA ###\n",
    "\n",
    "# Input test data\n",
    "test_01 = preprocess_data(ip_path + 'test.csv')\n",
    "\n",
    "\n",
    "# Scale test data\n",
    "scaler = MinMaxScaler()    \n",
    "test_scaled_01 = scaler.fit_transform(test_01.iloc[:, :11].values)\n",
    "\n",
    "\n",
    "# Get Actual vs Predicted values\n",
    "df_tr_pred_act, times = getPredvsAct(test_scaled_01)\n",
    "\n",
    "\n",
    "# Get Prediction Intervals\n",
    "df_pred_act_index_01, residuals = predIntervals(df_tr_pred_act, alpha, times) # get wave height predictions w/ PI\n",
    "\n",
    "\n",
    "# Prepare test set for extreme event prediction\n",
    "rfIP = df_pred_act_index_01.copy()\n",
    "rfIP['Extreme_Forecast'] = np.where(rfIP['WVHT_pred'] > exthresh, 1, 0)\n",
    "rfIP['Extreme_Forecast'] = rfIP['Extreme_Forecast'].astype('category') # convert forecasted wvht as categorical\n",
    "\n",
    "rfIP['Extreme_True'] = np.where(rfIP['WVHT_act'] > exthresh, 1, 0)\n",
    "rfIP['Extreme_True'] = rfIP['Extreme_True'].astype('category') # convert actual wvht as categorical\n",
    "\n",
    "rfFeatures = test_01.copy()\n",
    "\n",
    "\n",
    "",
    "merged_rfIP = pd.merge(left = rfIP, right = rfFeatures.iloc[:, 1 : rfFeatures.shape[1]], \n",
    "                       left_index = True, right_index = True)\n",
    "\n",
    "merged_rfIP = merged_rfIP.rename(columns={'WVHT_pred':'WVHT'})   \n",
    "\n",
    "for col in feature_list: \n",
    "        scaler = MinMaxScaler()\n",
    "        merged_rfIP[col] = scaler.fit_transform(merged_rfIP[col].values.reshape(-1,1))\n",
    "        \n",
    "        \n",
    "# Get Extreme Event Predictions from the forecasted values\n",
    "unseen_predictions = _extClassifier.predict(merged_rfIP[feature_list]) \n",
    "\n",
    "# Get Extreme Event Prediction Scores/Metrics\n",
    "unseen_ras = roc_auc_score(merged_rfIP['Extreme_True'], \n",
    "                           _extClassifier.predict_proba(merged_rfIP[feature_list])[:,1])\n",
    "\n",
    "precision_recall_fscore = precision_recall_fscore_support(merged_rfIP['Extreme_True'].values, \n",
    "                                                              unseen_predictions, \n",
    "                                                              average='weighted')\n",
    "accs_unseen = accuracy_score(merged_rfIP['Extreme_True'], unseen_predictions)\n",
    "\n",
    "cfm_unseen = confusion_matrix(merged_rfIP['Extreme_True'], unseen_predictions)\n",
    "\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in cfm_unseen.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in cfm_unseen.flatten()/np.sum(cfm_unseen)]\n",
    "\n",
    "group_percentages_float = [cfm_unseen.flatten()/np.sum(cfm_unseen)]\n",
    "\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "        \n",
    "# Save Predictions\n",
    "preds_op = pd.concat([merged_rfIP['Extreme_True'].reset_index(), \n",
    "                      pd.DataFrame(unseen_predictions, columns = [str(future_steps)])], \n",
    "                      axis=1)\n",
    "# preds_op.to_csv(op_path + str(future_steps)+'h_Ext_Predictions.csv')\n",
    "\n",
    "    \n",
    "# Compile metrics as a dictionary\n",
    "exPrediction_metrics = {'ROC_AUC Score': round(unseen_ras, 5), \n",
    "             'F1-Score': round(precision_recall_fscore[2], 5), \n",
    "             'Accuracy': round(accs_unseen, 5),\n",
    "             'Precision': round(precision_recall_fscore[0], 5), \n",
    "             'Recall': round(precision_recall_fscore[1], 5),\n",
    "             'FPR': round(group_percentages_float[0][1], 5), \n",
    "             'FNR': round(group_percentages_float[0][2], 5)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1691edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Actual vs Forecasted wave height values from test set\n",
    "\n",
    "forecastMetrics = generate_plot(df_pred_act_index_01, residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get forecast metrics for test data\n",
    "\n",
    "forecastMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Extreme Event Prediction Confusion Matrix for test data\n",
    "\n",
    "plotExtCM(cfm_unseen, future_steps, labels, op_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get extreme event prediction metrics for test data\n",
    "\n",
    "exPrediction_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f6f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463fb719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
